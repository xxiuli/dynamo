import os
import torch
from transformers import AutoModel

# æ¯ä¸ª adapter å¯¹åº”çš„åŸå§‹ backbone æ¨¡å‹å
adapter_backbones = {
    "adapter_agnews": "distilbert-base-uncased",
    "adapter_sst2": "cardiffnlp/twitter-roberta-base-sentiment",
    "adapter_mnli": "roberta-large-mnli",
    "adapter_qqp": "bert-base-uncased",
    "adapter_conll03": "dslim/bert-base-NER",
    "adapter_squad": "deepset/roberta-large-squad2",
    "adapter_xsum": "google/pegasus-xsum"
}

# è·¯å¾„ï¼šè¯·æ ¹æ®ä½ çš„æœ¬åœ°ç»“æ„è‡ªå®šä¹‰
CHECKPOINT_DIR = r"C:\Users\xiuxiuli.SSNC-CORP\Desktop\learn\567ML\dynamo\DynamoRouterCheckpoints"

def fix_backbone(adapter_dir, backbone_name):
    adapter_path = os.path.join(CHECKPOINT_DIR, adapter_dir)
    model_path = os.path.join(adapter_path, "pytorch_model.bin")

    print(f"\nğŸ”§ Checking {adapter_dir}...")

    # å¦‚æœå·²ç»å­˜åœ¨ï¼Œè·³è¿‡
    if os.path.exists(model_path):
        print("âœ… pytorch_model.bin already exists. Skipping.")
        return

    # å°è¯•åŠ è½½å¹¶ä¿å­˜ transformer ä¸»æ¨¡å‹æƒé‡
    try:
        print(f"â³ Saving backbone: {backbone_name}")
        model = AutoModel.from_pretrained(backbone_name)
        torch.save(model.state_dict(), model_path)
        print("âœ… Saved pytorch_model.bin")
    except Exception as e:
        print(f"âŒ Failed to save model for {adapter_dir}: {e}")

def main():
    for adapter_name, backbone in adapter_backbones.items():
        fix_backbone(adapter_name, backbone)

if __name__ == "__main__":
    main()
