from utils.setting_utils import load_config
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from router.router_classifier import RouterClassifier
from model.custom_cls_model import CustomClassificationModel
from peft import PeftModel
import torch
import torch.nn.functional as F

def load_router(router_cfg):
    # ä½¿ç”¨ router.tokenizer åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(router_cfg['tokenizer'])

    # åˆå§‹åŒ– Router æ¨¡å‹ç»“æ„
    model = RouterClassifier.from_pretrained(router_cfg['checkpoint_path'])

    return tokenizer, model

def get_all_adapters(tasks, device):
    target_adapters = {}
    tokenizer_cache = {}

    for task_name, task_cfg in tasks.items():
        adapter_cfg = load_config(task_cfg['config_path'])
        trained_lora_dir = task_cfg['adapter_path']

        # tokenizer ç¼“å­˜å¤ç”¨
        if trained_lora_dir in tokenizer_cache:
            tokenizer = tokenizer_cache[trained_lora_dir]
        else:
            tokenizer = AutoTokenizer.from_pretrained(trained_lora_dir)
            tokenizer_cache[trained_lora_dir] = tokenizer

        model = load_adapter_model(task_cfg, adapter_cfg, device)

        target_adapters[task_name] = {
                "tokenizer": tokenizer,
                "model": model
            }

    return target_adapters

def load_adapter_model(task_cfg, adapter_cfg, device):
    task_type = task_cfg['task_type'].lower()
    model_dir = task_cfg['adapter_path']

    if task_type == 'classification':
        # ${DRIVE_ROOT}/DynamoRouterCheckpoints/adapter_agnews
        model = CustomClassificationModel.from_pretrained(
            model_dir,
            num_labels=adapter_cfg['num_labels']
            ).to(device)
        return model

    elif task_type in ['qa', 'summarization']:
        base_model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)
        model = PeftModel.from_pretrained(base_model, model_dir).to(device)
        model.eval()
        return model
    else:
        raise ValueError(f"[ERROR] Unknown task type: {task_type}")
    
def preprocess_data(text: str, task_type: str, tokenizer):
    if task_type == "classification":
        return tokenizer(text, return_tensors="pt")
    elif task_type == "qa":
        # æš‚ç”¨æ¨¡æ¿ï¼ˆåç»­å»ºè®®ç”±çœŸå®é—®ç­”ç»“æ„ä»£æ›¿ï¼‰
        return tokenizer({"question": "What is X?", "context": text}, return_tensors="pt")
    elif task_type == "summarization":
        return tokenizer(text, return_tensors="pt")
    else:
        raise ValueError(f"[ERROR] Unknown task type: {task_type}")

class Dynamo:
    def __init__(self, config: str):
        self.router, self.tokenizer = load_router(config["router"])
        self.tasks = config["tasks"]
        self.target_adapters = get_all_adapters(self.tasks)

    def predict(self, text: str, top_k: int = 3) -> dict:
        # Step 1: ç”¨ Router åˆ†å‘ä»»åŠ¡
        inputs = self.tokenizer(text, return_tensors='pt')
        logits = self.router(**inputs)
        probs = F.softmax(logits, dim=-1)

        task_idx = torch.argmax(probs, dim=1).item()

        # æ‰¾åˆ°ä»»åŠ¡åï¼ˆä¿æŒé¡ºåºä¸€è‡´ï¼‰
        task_name = list(self.tasks.keys())[task_idx]
        task_cfg = self.tasks[task_name]

        adapter_info = self.target_adapters[task_name]
        adapter_tokenizer = adapter_info["tokenizer"]
        adapter_model = adapter_info["model"]

        # æ‰“å° Router logits å’Œæ¦‚ç‡
        print("\nğŸ“Š Router logits:", logits.tolist())
        print("ğŸ“ˆ Router softmax:", probs.tolist())

         # è¾“å‡º Top-k è·¯ç”±å€™é€‰
        top_probs, top_indices = probs[0].topk(k=min(top_k, len(self.tasks)))
        top_k_results = [
            {
                "task": list(self.tasks.keys())[i],
                "confidence": round(top_probs[j].item(), 4)
            }
            for j, i in enumerate(top_indices)
        ]

        # Step 2: Adapter æ¨ç†
        adapter_info = self.target_adapters[task_name]
        adapter_tokenizer = adapter_info["tokenizer"]
        adapter_model = adapter_info["model"]

        task_type = task_cfg["task_type"].lower()
        task_inputs = preprocess_data(text, task_type, adapter_tokenizer)
        task_inputs = {k: v.to(self.device) for k, v in task_inputs.items()}

        # Step 3: æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = adapter_model(**task_inputs)

        # Step 4: è§£ç ç»“æœ
        if task_type == "classification":
            if task_type == "summarization":
                pred_ids = adapter_model.generate(**task_inputs)
                pred = adapter_tokenizer.decode(pred_ids[0], skip_special_tokens=True)

            elif task_type == "qa":
                outputs = adapter_model(**task_inputs)
                start_idx = torch.argmax(outputs.start_logits, dim=1)
                end_idx = torch.argmax(outputs.end_logits, dim=1)
                pred_tokens = task_inputs["input_ids"][0][start_idx: end_idx + 1]
                pred = adapter_tokenizer.decode(pred_tokens, skip_special_tokens=True)

            elif task_type == "classification":
                outputs = adapter_model(**task_inputs)
                pred = torch.argmax(outputs.logits, dim=-1).item()

            else:
                pred = "Unsupported task type"


        return {
            "text": text,
            "task": task_name,
            "task_type": task_type,
            "predicted_label": pred,
            "top_k_router": top_k_results
        }


    


    