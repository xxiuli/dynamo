from model.custom_token_model import CustomTokenClassificationModel
from utils.setting_utils import load_config
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoModelForSeq2SeqLM
from router.router_classifier import RouterClassifier
from model.custom_cls_model import CustomClassificationModel
from peft import PeftModel
import torch
import torch.nn.functional as F
from utils.setting_utils import apply_path_placeholders
import os
from utils.task_id_map import get_id2task, get_task2id

def load_router(router_cfg):
    # ä½¿ç”¨ router.tokenizer åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(router_cfg['tokenizer'])

    # åˆå§‹åŒ– Router æ¨¡å‹ç»“æ„
    model = RouterClassifier.from_pretrained(router_cfg['checkpoint_path'])

    return tokenizer, model

def get_all_adapters(tasks, device):
    target_adapters = {}
    tokenizer_cache = {}
    
    for task_name, task_cfg in tasks.items():
        adapter_cfg = load_config(task_cfg['config_path'])
        adapter_cfg = apply_path_placeholders(adapter_cfg)

        trained_lora_dir = task_cfg['adapter_path']

        # tokenizer ç¼“å­˜å¤ç”¨
        if trained_lora_dir in tokenizer_cache:
            tokenizer = tokenizer_cache[trained_lora_dir]
        else:
            tokenizer = AutoTokenizer.from_pretrained(trained_lora_dir)
            tokenizer_cache[trained_lora_dir] = tokenizer

        model = load_adapter_model(task_cfg, adapter_cfg, device)

        target_adapters[task_name] = {
                "tokenizer": tokenizer,
                "model": model
            }

    return target_adapters

def load_adapter_model(task_cfg, adapter_cfg, device):
    task_type = task_cfg['task_type'].lower()

    if task_type == 'classification':
        # ${DRIVE_ROOT}/DynamoRouterCheckpoints/adapter_agnews
        model = CustomClassificationModel.from_pretrained(
            task_cfg['model_paths'],
            num_labels=adapter_cfg['num_labels']
            ).to(device)
        return model
    
    elif task_type == 'ner':
        model = CustomTokenClassificationModel.from_pretrained(
            task_cfg['model_paths'],
            num_labels=adapter_cfg['num_labels']
        ).to(device)
        return model

    elif task_type == 'qa':
        # é—®ç­”ä»»åŠ¡ï¼šä½¿ç”¨ AutoModelForQuestionAnswering
        base_model = AutoModelForQuestionAnswering.from_pretrained(
            task_cfg['adapter_path']
        ).to(device)

        model = PeftModel.from_pretrained(base_model, task_cfg['adapter_path']).to(device)
        model.eval()
        return model

    elif task_type == 'summarization':
        # æ‘˜è¦ä»»åŠ¡ï¼šä½¿ç”¨ Seq2Seq æ¨¡å‹æ¶æ„
        base_model = AutoModelForSeq2SeqLM.from_pretrained(
            task_cfg['adapter_path']
        ).to(device)

        model = PeftModel.from_pretrained(base_model, task_cfg['adapter_path']).to(device)
        model.eval()
        return model
    else:
        raise ValueError(f"[ERROR] Unknown task type: {task_type}")
    
def preprocess_data(text: str, task_type: str, tokenizer):
    if task_type == "classification":
        return tokenizer(text, return_tensors="pt")
    elif task_type == "qa":
        # æš‚ç”¨æ¨¡æ¿ï¼ˆåç»­å»ºè®®ç”±çœŸå®é—®ç­”ç»“æ„ä»£æ›¿ï¼‰
        return tokenizer({"question": "What is X?", "context": text}, return_tensors="pt")
    elif task_type == "summarization":
        return tokenizer(text, return_tensors="pt")
    else:
        raise ValueError(f"[ERROR] Unknown task type: {task_type}")
    
def build_path(task_name, task_cfg):
    adapter_dir = task_cfg['adapter_path']
    paths = {
        "config": os.path.join(adapter_dir, "config.json"),
        "model": os.path.join(adapter_dir, "pytorch_model.bin"),
        "head": os.path.join(adapter_dir, "head.pth"),
        "adapter_weight": os.path.join(adapter_dir, f"adapter_{task_name}.safetensors"),
    }
    return paths

class Dynamo:
    def __init__(self, config: str):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer, self.router = load_router(config["router"])
        self.tasks = config["tasks"]
        self.target_adapters = get_all_adapters(self.tasks, self.device)
        self.task_id_map = get_task2id()
        self.id_task_map = get_id2task()
        self.temperature = config['router']['temperature']

    def predict(self, text: str, top_k: int = 3) -> dict:
        # Step 1: ç”¨ Router åˆ†å‘ä»»åŠ¡
        inputs = self.tokenizer(text, return_tensors='pt')
        input_ids = inputs['input_ids'].to(self.device)
        attention_mask = inputs['attention_mask'].to(self.device)

        logits = self.router(input_ids=input_ids, attention_mask=attention_mask)    # ğŸ› ï¸ åªä¼  input_ids ç»™ Router
        probs = F.softmax(logits/ self.temperature, dim=-1)

        task_idx = torch.argmax(probs, dim=1).item()

        # æ‰¾åˆ°ä»»åŠ¡åï¼ˆä¿æŒé¡ºåºä¸€è‡´ï¼‰
        task_name = self.id_task_map[task_idx]

        print(f"\nğŸ“Š Router è®¤ä¸ºè¿™æ˜¯:{task_name} - {task_idx }ä»»åŠ¡")
        task_cfg = self.tasks[task_name]

        adapter_info = self.target_adapters[task_name]
        adapter_tokenizer = adapter_info["tokenizer"]
        adapter_model = adapter_info["model"]

        # æ‰“å° Router logits å’Œæ¦‚ç‡
        print("\nğŸ“Š Router logits:", logits.tolist())
        print("ğŸ“ˆ Router softmax:", probs.tolist()) 

         # è¾“å‡º Top-k è·¯ç”±å€™é€‰
        top_probs, top_indices = probs[0].topk(k=min(top_k, len(self.tasks)))
        top_k_results = [
            {
                "task": self.id_task_map[i.item()],
                "confidence": round(top_probs[j].item(), 4)
            }
            for j, i in enumerate(top_indices)
        ]

        # Step 2: Adapter æ¨ç†
        task_type = task_cfg["task_type"].lower()
        task_inputs = preprocess_data(text, task_type, adapter_tokenizer)
        task_inputs = {k: v.to(self.device) for k, v in task_inputs.items()}
        print(f"[ğŸ§ª] è¾“å…¥æ¨¡å‹çš„å­—æ®µ: {list(task_inputs.keys())}")


        # Step 3: æ¨¡å‹æ¨ç† # Step 4: è§£ç ç»“æœ
        with torch.no_grad():
            if task_type == "summarization":
                print("[ğŸ§  LORA Adapter] ä½¿ç”¨ generate è¿›è¡Œ summarization æ¨ç†")
                for key in ["decoder_input_ids", "decoder_inputs_embeds"]:
                    if key in task_inputs:
                        print(f"[âš ï¸] ç§»é™¤å†²çªå­—æ®µï¼š{key}")
                        del task_inputs[key]
                pred_ids = adapter_model.generate(
                    input_ids=task_inputs["input_ids"],
                    attention_mask=task_inputs.get("attention_mask"),
                    max_length=60,
                    num_beams=4,
                    early_stopping=True
                )
                pred = adapter_tokenizer.decode(pred_ids[0], skip_special_tokens=True).strip()
                print(f"ğŸ“¤ Adapterè¾“å‡ºï¼ˆæ‘˜è¦ï¼‰: {pred}")
            
            elif task_type == "qa":
                print("[ğŸ§  LORA Adapter] æ‰§è¡Œé—®ç­”ä»»åŠ¡ï¼ˆextractive QAï¼‰")
                outputs = adapter_model(**task_inputs)
                start_idx = torch.argmax(outputs.start_logits, dim=1)
                end_idx = torch.argmax(outputs.end_logits, dim=1)
                pred_tokens = task_inputs["input_ids"][0][start_idx: end_idx + 1]
                pred = adapter_tokenizer.decode(pred_tokens, skip_special_tokens=True)
                print(f"ğŸ“¤ Adapterè¾“å‡ºï¼ˆç±»åˆ«ç´¢å¼•ï¼‰: {pred}")

            elif task_type == "classification":
                print("[ğŸ§  LORA Adapter] æ‰§è¡Œclassificationåˆ†ç±»ä»»åŠ¡")
                outputs = adapter_model(**task_inputs)
                pred = torch.argmax(outputs.logits, dim=-1).item()
                print(f"ğŸ“¤ Adapterè¾“å‡ºï¼ˆç±»åˆ«ç´¢å¼•ï¼‰: {pred}")

            elif task_type == "ner":
                print("[ğŸ§  LORA Adapter] æ‰§è¡Œnerå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡")
                outputs = adapter_model(**task_inputs)  # logits: [1, seq_len, num_labels]
                predicted_ids = torch.argmax(outputs, dim=-1)  # [1, seq_len]
                tokens = adapter_tokenizer.convert_ids_to_tokens(task_inputs["input_ids"][0])
                labels = [adapter_model.config.id2label[idx.item()] for idx in predicted_ids[0]]
                
                pred = list(zip(tokens, labels))  # token-label pair
                preview = list(zip(tokens, labels))[:10]
                print(f"ğŸ“¤ LORA Adapterè¾“å‡ºï¼ˆå‰10å¯¹ token-labelï¼‰: {preview}")

            else:
                pred = "Unsupported task type"

        return {
            "text": text,
            "task": task_name,
            "task_id": task_idx,
            "predicted_label": pred,
            "top_k_router": top_k_results
        }


    


    