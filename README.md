
---

# DynamoRouter: A Multi-Task LoRA Dynamic Routing Transformer System

ğŸš€ **DynamoRouter** is a multi-task learning system built on **RoBERTa + LoRA adapters + dynamic task routing**.
It supports **task-specific fine-tuning** with **parameter-efficient PEFT methods**, and performs **end-to-end dynamic routing across multiple NLP tasks**.

---

## ğŸ§± Project Architecture Overview

```
[ Input Text ]
      â”‚
[ Frozen RoBERTa Encoder ]
      â”‚
[ Task Router (MLP) ]
      â”‚
[ Selected LoRA Adapter ] â†’ [ Task-Specific Output Head (Classification / QA / Summarization) ]
      â”‚
[ Loss Calculation (Per Task) ]
```

---

## ğŸ¯ Supported Tasks (Current Setup)

| Task  | Dataset                                                             | LoRA TaskType               |
| ----- | ------------------------------------------------------------------- | --------------------------- |
| SST-2 | Sentiment Classification                                            | SEQ\_CLS                    |
| SQuAD | Question Answering (Span Prediction)                                | QUESTION\_ANS               |
| XSum  | Summarization / Binary Classification (depending on implementation) | SEQ\_CLS or SEQ\_2\_SEQ\_LM |

> âœ… Easily extendable for more tasks!

---

## âœ… Features

* âœ… **Multi-Task Learning (MTL)**
* âœ… **Task-Specific LoRA Adapters**
* âœ… **Dynamic Task Router (MLP-based)**
* âœ… **Multi-Head End-to-End Training**
* âœ… **Multi-Loss Support**
* âœ… **Config-Driven Training Pipelines**
* âœ… **Google Colab + Drive Integration**
* âœ… **PEFT (Parameter-Efficient Fine-Tuning)**

---

## ğŸ“‚ Project Structure

```
DynamoRouter/
â”œâ”€â”€ configs/                  # LoRA and training hyperparameter configs
â”‚     â”œâ”€â”€ lora_sst2.json
â”‚     â”œâ”€â”€ lora_squad.json
â”‚     â””â”€â”€ lora_xsum.json
â”œâ”€â”€ data/
â”‚     â””â”€â”€ raw/                 # Raw datasets (JSONL or JSON format)
â”œâ”€â”€ lora_checkpoints/          # Saved adapter weights
â”œâ”€â”€ router_checkpoints/        # Saved Router MLP weights
â”œâ”€â”€ end2end_checkpoints/       # Full End-to-End training checkpoints
â”œâ”€â”€ train_lora.py              # Single-task LoRA trainer (config-driven)
â”œâ”€â”€ train_router.py            # Router trainer
â”œâ”€â”€ train_end2end.py           # Multi-head End-to-End trainer
â””â”€â”€ README.md
```

---

## âš™ï¸ Training Workflow

| Phase                              | Script             | Description                                      |
| ---------------------------------- | ------------------ | ------------------------------------------------ |
| 1ï¸âƒ£ Single-task LoRA fine-tuning   | `train_lora.py`    | Fine-tune one adapter per task                   |
| 2ï¸âƒ£ Router Training                | `train_router.py`  | Train MLP router on mixed task data              |
| 3ï¸âƒ£ End-to-End Multi-Task Training | `train_end2end.py` | Joint training of router + adapters (multi-head) |

---

## ğŸ“Œ Example Usage (Colab)

### Mount Google Drive:

```python
from google.colab import drive
drive.mount('/content/drive')
```

### Train SST2 LoRA Adapter:

```bash
python train_lora.py --config_path ./configs/lora_sst2.json
```

### Train Router:

```bash
python train_router.py
```

### End-to-End Training:

```bash
python train_end2end.py
```

---

## ğŸ“ Possible Improvements (Future Work)

* âœ… AdapterFusion / AdapterDrop support
* âœ… Weighted multi-task loss
* âœ… Optuna / W\&B hyperparameter search
* âœ… Model Export for production inference
* âœ… Docker deployment / API serving

---

## ğŸ§‘â€ğŸ’» Author

**Xiuxiu Li**
*ML Engineer / Full-Stack Developer / MLOps Learner*

---

