import yaml
import torch
import json
from transformers import(AutoModel, AutoTokenizer)
from torch.utils.data import Dataset, DataLoader
from peft import get_peft_model, LoraConfig, TaskType
from torch import nn
from types import SimpleNamespace
from utils.train_utils import set_seed, freeze_base_model, print_trainable_params
import trainer_cls

#tensorboard --logdir=runs, ç„¶åæ‰“å¼€æµè§ˆå™¨è®¿é—®ï¼šhttp://localhost:6006

CONFIG_PATH = "configs/single_lora_sst2.yaml"

class TextClassificationDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_seq_len):
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.samples = []

        with open(file_path, 'r') as f:
            try:
                for line in f:
                    item = json.loads(line)
                    self.samples.append({
                        'text': item['sentence'],
                        'label': item['label']
                    })
            except json.JSONDecodeError as e:
                raise ValueError(f"[ERROR] JSON decoding failed in {file_path}: {e}")

        if not self.samples:
            raise ValueError(f"[ERROR] Dataset is empty: {file_path}")

    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        item = self.samples[idx]
        encoding = self.tokenizer(
            item['text'], 
            truncation=True,
            padding='max_length', 
            max_length = self.max_seq_len,
            return_tensors = 'pt'
        )
        return{
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(item['label'], dtype=torch.long)
        }
    
def load_config(path):
    try:
        with open(path, 'r') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"[ERROR] Config file not found: {path}")
    except yaml.YAMLError as e:
        raise ValueError(f"[ERROR] Invalid YAML format: {e}")


def main():
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼Œ
    config = load_config(CONFIG_PATH)

    # è®¾ç½®ç§å­ï¼Œé€‰æ‹© GPU/CPU
    set_seed(config['train']['seed'])
    device = torch.device("cuda" if torch.cuda.is_available() else 'cpu')
    
    try: 
        # AutoTokenizer æ˜¯æŠŠæ–‡æœ¬ï¼ˆå¥å­ï¼‰è½¬ä¸ºæ¨¡å‹è¾“å…¥æ‰€éœ€çš„ token åºåˆ—. tokenizeï¼ˆåˆ†è¯ + ç¼–ç ï¼‰
        tokenizer = AutoTokenizer.from_pretrained(config['backbone_model']) # roberta-base
        # load base model 1. åŠ è½½æ¨¡å‹
        base_model = AutoModel.from_pretrained(config['backbone_model']) # roberta-base
    except Exception as e:
        raise RuntimeError(f"[ERROR] Failed to load backbone model/tokenizer: {e}")
    
    train_set = TextClassificationDataset(config['data']['train_file'], tokenizer, config['train']['max_seq_length'])
    val_set = TextClassificationDataset(config['data']['val_file'], tokenizer, config['train']['max_seq_length'])

    # å­˜å…¥LOADERï¼Œ total/batchsize, åˆ†å¼€å¾ˆå¤šä¸ªå°LOAD | æŠŠæ•°æ®é›†æŒ‰ batch åˆ’åˆ†ï¼Œå°è£…æˆè¿­ä»£å™¨ä¾›è®­ç»ƒæ—¶ä½¿ç”¨
    train_loader = DataLoader(train_set, batch_size=config['train']['batch_size'], shuffle=True)
    val_loader = DataLoader(val_set, batch_size=config['train']['batch_size'])
    
    # inject lora 2. æ³¨å…¥ LoRA
    peft_config = LoraConfig(
        task_type = TaskType.SEQ_CLS,
        r = config['lora']['r'],
        lora_alpha = config['lora']['alpha'],
        lora_dropout = config['lora']['dropout'],
        bias = 'none',
        target_modules=config['lora']['target_modules']
    )
    
    try:
        # æŠŠLORA æ’å…¥ROBERTA
        model = get_peft_model(base_model, peft_config)
        model.to(device)
    except Exception as e:
        raise RuntimeError(f"[ERROR] Failed to inject LoRA: {e}")

    # Add classification head 3. æ·»åŠ åˆ†ç±»å¤´
    # hidden_size = base_model.config.hidden_size
    # num_labels = config['num_labels']

    # åˆå§‹åŒ–â€™LORA+ROBERTAâ€˜åˆä½“
    # model = LoRAModelWithClassifier(lora_base, hidden_size, num_labels)

    # 4. å†»ç»“ä¸»å¹²ï¼ˆä¸€å®šè¦åœ¨ optimizer ä¹‹å‰ï¼‰
    freeze_base_model(model)

    # 5. æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°ç›®ï¼ˆå¯é€‰ï¼‰
    print_trainable_params(model)

    # æ¨¡å‹å’Œæ•°æ®éƒ½è¦ç§»åˆ°è®¾å¤‡ä¸Š
    # model.to(device)

    # è‡ªåŠ¨è®°å½• steps_per_epoch
    config['train']['steps_per_epoch'] = len(train_loader)

    #å¼€å§‹è®­ç»ƒ
    trainer_instance = trainer_cls.SingleTaskTrainer(model, config, device)
    try:
        trainer_instance.train(train_loader, val_loader, tokenizer=tokenizer)
    except Exception as e:
        print(f"[ERROR] Training failed: {e}")
        raise  # å¯é€‰ï¼Œæˆ–ä¿å­˜æ—¥å¿—å†é€€å‡º
    

    print(f"\nğŸš€ TensorBoard started! Run this command to view logs:\n")
    print(f"   tensorboard --logdir={config['output']['log_dir']}")
    print("Then open http://localhost:6006 in your browser.\n")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[FATAL] Uncaught exception in main: {e}")