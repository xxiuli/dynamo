from typing import Optional
from transformers import AutoModel,AutoConfig
from heads.classification_head import ClassificationHead
import torch.nn as nn
from transformers.modeling_outputs import SequenceClassifierOutput
import os
import torch
from safetensors.torch import load_file  # âœ… å¯¼å…¥ safetensors loader


class CustomClassificationModel(nn.Module):
    # def __init__(self, backbone, num_labels, ignore_mismatched_sizes=False):
    def __init__(
            self, 
            backbone: Optional[nn.Module] = None, 
            backbone_dir: Optional[str] = None,
            num_labels: int = 2, 
            ignore_mismatched_sizes: bool=False
        ):

        super().__init__()

        if backbone is not None and backbone_dir is not None:
            raise ValueError("åªèƒ½æä¾› backbone æˆ– backbone_dir ä¸­çš„ä¸€ä¸ªï¼Œä¸å¯åŒæ—¶æä¾›ã€‚")
        
        if backbone is not None:
            self.backbone = backbone
        elif backbone_dir is not None:
            self.backbone = AutoModel.from_pretrained(
                backbone_dir,
                ignore_mismatched_sizes=ignore_mismatched_sizes,
                local_files_only=True # âœ… é›†æˆæ—¶å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°
            )
        else:
            raise ValueError("å¿…é¡»æä¾› backbone æˆ– backbone_dir å…¶ä¸­ä¹‹ä¸€ã€‚")
      
        self.config = self.backbone.config #LoRA çš„æ—¶å€™ï¼ŒPeftModel ä½“ç³»ä¼šå°è¯•è®¿é—® .config.use_return_dict
        
        hidden_size = self.backbone.config.hidden_size #ä»é¢„è®­ç»ƒæ¨¡å‹ config ä¸­è¯»å–è¾“å‡ºç»´åº¦
        # è¦ç¡®ä¿ RoBERTa çš„ hidden size å’Œ Head çš„è¾“å…¥ç»´åº¦ä¸€æ ·, æœ€å…³é”®çš„æ¥å£ä¸€è‡´æ€§åŸåˆ™
        self.head = ClassificationHead(hidden_size, num_labels)

    # æ¨¡å‹æ•´ä½“çš„ forward , æŠŠæ•°æ®FORWARDå‘HEAD
    def forward(self, input_ids, attention_mask=None, labels=None, inputs_embeds=None, **kwargs): 
    # def forward(self, input_ids, attention_mask=None,inputs_embeds=None, **kwargs): 

        # Step 1: è¾“å…¥è¿›å…¥ RoBERTaï¼ˆæˆ– BERTï¼‰
        outputs = self.backbone(
            input_ids=input_ids, 
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,  # âœ… æ·»åŠ è¿™è¡Œï¼Œè½¬å‘ç»™ backbone
            **kwargs
            ) # â†’ Roberta

        # Step 2: æ‹¿åˆ° [CLS] token å‘é‡ï¼ˆç¬¬ä¸€ä¸ªä½ç½®ï¼‰
        sequence_output = outputs.last_hidden_state           # [batch_size, seq_len, hidden_size] 
        pooled_output = sequence_output[:, 0, :]  # âœ… å– [CLS] å‘é‡
        # Step 3: è¾“å…¥åˆ°åˆ†ç±»å¤´ï¼ˆHead ä¼šè‡ªåŠ¨è°ƒç”¨å®ƒçš„ forwardï¼‰
        # logits = æ¨¡å‹è¾“å‡ºçš„æ¯ä¸ªç±»åˆ«çš„åŸå§‹åˆ†æ•°ï¼ˆæœªå½’ä¸€åŒ–ï¼‰
        logits = self.head(pooled_output)             # [batch_size, seq_len, num_labels]     

        loss = None

        # Step 4: å¦‚æœæœ‰æ ‡ç­¾ï¼Œè®¡ç®—æŸå¤±
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss(ignore_index=-100) #éœ€è¦å±•å¹³
            # ç»SOFTMAX å½’ä¸€åŒ–
            # loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
            loss = loss_fn(logits, labels)
        
        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions
        )
    
    def save_pretrained(self, save_directory):
        os.makedirs(save_directory, exist_ok=True)

        # âœ… å¦‚æœæ˜¯ PEFT æ¨¡å‹ï¼Œä¿å­˜ base_model.model
        if hasattr(self.backbone, "base_model") and hasattr(self.backbone.base_model, "model"):
            self.backbone.base_model.model.save_pretrained(save_directory)
            print(f"[âœ”] PEFT base_model saved to {save_directory}")
        elif hasattr(self.backbone, "save_pretrained"):
            self.backbone.save_pretrained(save_directory)
            print(f"[âœ”] Backbone saved to {save_directory}")
        else:
            print("[âš ï¸] No valid backbone.save_pretrained found.")

        # å¦å¤–ä¿å­˜ä½ è‡ªå®šä¹‰çš„ head
        torch.save(self.head.state_dict(), os.path.join(save_directory, "head.pth"))
        
        # ä¿å­˜ config
        with open(os.path.join(save_directory, "config.json"), "w") as f:
            f.write(self.config.to_json_string(indent=2))  # å¯è¯»æ€§æ›´å¥½
    
    @classmethod
    def from_pretrained(cls, paths, num_labels=None):
    # def from_pretrained(cls, load_directory, num_labels=None):
        print(f"ğŸŸ¢ [CustomModel] Loading from {paths}")

        config_path = paths['config']
        model_path = paths['model']
        adapter_weights_path = paths['adapter_weight']

        # âœ… Step 1: åŠ è½½ transformer config
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"âŒ Missing config.json in {config_path}")
        config = AutoConfig.from_pretrained(config_path)

        # âœ… Step 2: æ„é€  backbone æ¨¡å‹ç»“æ„
        backbone = AutoModel.from_config(config)

        # âœ… Step 3: åŠ è½½ä¸»æ¨¡å‹æƒé‡
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ Missing pytorch_model.bin in {model_path}")
        state_dict = torch.load(model_path, map_location="cpu")
        backbone.load_state_dict(state_dict)

        print("âœ… Loaded backbone from config + weights")

        # âœ… Step 4: è·å– num_labels
        config_num_labels = getattr(config, 'num_labels', None)
        effective_num_labels = num_labels if num_labels is not None else (config_num_labels or 2)

        # âœ… Step 5: æ„å»ºè‡ªå®šä¹‰åˆ†ç±»æ¨¡å‹
        model = cls(backbone=backbone, num_labels=effective_num_labels)

        model.config = config  # æ›´æ–° configï¼ˆå¦åˆ™ç”¨çš„æ˜¯æ—§çš„ï¼‰

        # âœ… Step 6: åŠ è½½ adapterï¼ˆLoRAï¼‰
        if os.path.exists(adapter_weights_path):
            print("ğŸ§© Loading LoRA adapter weights...")
            model.load_state_dict(load_file(adapter_weights_path), strict=False)
        else:
            print("âš ï¸ adapter_model.safetensors not found, skipping LoRA load")

         # âœ… Step 7: åŠ è½½è‡ªå®šä¹‰çš„åˆ†ç±»å¤´ï¼ˆHeadï¼‰
        head_path = paths["head"]
        if head_path and os.path.exists(head_path):
            print("ğŸ§  Loading head weights from:", head_path)
            model.head.load_state_dict(torch.load(head_path, map_location="cpu"))
        else:
            print("âš ï¸ head.pth not found, using randomly initialized head.")

        return model

    # @classmethod #å‘Šè¯‰ Python è¿™ä¸ªæ–¹æ³•æ˜¯ç±»æ–¹æ³•ï¼Œä¸æ˜¯å®ä¾‹æ–¹æ³•
    # def from_pretrained(cls, load_directory, num_labels=None):
    #     # 1. åŠ è½½ backboneï¼ˆåŒ…æ‹¬ configï¼‰
    #     print(f'Check dir:  {load_directory}')

    #     backbone = AutoModel.from_pretrained(load_directory, local_files_only=True)
    #     config = backbone.config

    #     # å°è¯•ä» config ä¸­è¯» num_labels
    #     config_num_labels = getattr(config, 'num_labels', None)

    #     # ä½¿ç”¨ä¼˜å…ˆçº§ï¼šå¤–éƒ¨ä¼ å…¥ > config.json > fallback
    #     effective_num_labels = num_labels if num_labels is not None else (config_num_labels or 2)

    #     # 2. æ„å»ºæ¨¡å‹ï¼ˆç”¨æ¥è‡ªå®šä¹‰çš„åˆ†ç±»å¤´ç»´åº¦ï¼‰
    #     model = cls(backbone_dir=load_directory, num_labels=effective_num_labels)

    #     # 3. åŠ è½½è‡ªå®šä¹‰ Head
    #     head_path = os.path.join(load_directory, "head.pth")
    #     model.head.load_state_dict(torch.load(head_path, map_location="cpu"))

    #     # âœ… 5. åŠ è½½ LoRA adapter æƒé‡ï¼ˆQ/V å±‚ï¼‰
    #     adapter_weights_path = os.path.join(load_directory, "adapter_model.safetensors")
    #     if os.path.exists(adapter_weights_path):
    #         model.load_state_dict(torch.load(adapter_weights_path, map_location="cpu"), strict=False)

    #     return model

